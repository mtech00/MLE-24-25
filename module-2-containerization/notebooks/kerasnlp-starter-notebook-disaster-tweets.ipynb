{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee81198",
   "metadata": {
    "papermill": {
     "duration": 0.007664,
     "end_time": "2023-08-02T23:46:35.683443",
     "exception": false,
     "start_time": "2023-08-02T23:46:35.675779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
    "This starter notebook is provided by the Keras team.</center>\n",
    "\n",
    "## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n",
    "\n",
    "In this competition, the challenge is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n",
    "A dataset of 10,000 tweets that were hand classified is available. \n",
    "\n",
    "__This starter notebook uses the [DistilBERT](https://arxiv.org/abs/1910.01108) pretrained model from KerasNLP.__\n",
    "\n",
    "\n",
    "**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n",
    "\n",
    "The BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n",
    "\n",
    "**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n",
    "\n",
    "\n",
    "\n",
    "![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Load the Disaster Tweets\n",
    "- Explore the dataset\n",
    "- Preprocess the data\n",
    "- Load a DistilBERT model from Keras NLP\n",
    "- Train your own model, fine-tuning BERT\n",
    "- Generate the submission file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b18f6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 25.94399,
     "end_time": "2023-08-02T23:47:01.633877",
     "exception": false,
     "start_time": "2023-08-02T23:46:35.689887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install keras-core --upgrade\n",
    "#!pip install  keras-nlp --upgrade\n",
    "\n",
    "# This sample uses Keras Core, the multi-backend version of Keras.\n",
    "# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "!export TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d35715",
   "metadata": {
    "papermill": {
     "duration": 10.34155,
     "end_time": "2023-08-02T23:47:11.983170",
     "exception": false,
     "start_time": "2023-08-02T23:47:01.641620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40ed36",
   "metadata": {
    "papermill": {
     "duration": 0.007174,
     "end_time": "2023-08-02T23:47:11.997843",
     "exception": false,
     "start_time": "2023-08-02T23:47:11.990669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the Disaster Tweets\n",
    "Let's have a look at the train and test dataset.\n",
    "\n",
    "They contain:\n",
    "- id\n",
    "- keyword: A keyword from that tweet (although this may be blank!)\n",
    "- location: The location the tweet was sent from (may also be blank)\n",
    "- text: The text of a tweet\n",
    "- target: 1 if the tweet is a real disaster or 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46224f",
   "metadata": {
    "papermill": {
     "duration": 0.090102,
     "end_time": "2023-08-02T23:47:12.095177",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.005075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "#df = pd.read_csv('/app/data/tweets.csv')\n",
    "\n",
    "# Split: 80% train, 10% val, 10% test\n",
    "#df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#df_test = df_test.drop(columns=[\"target\"])\n",
    "\n",
    "df_train = pd.read_csv(\"/app/data/train.csv\")\n",
    "df_test = pd.read_csv(\"/app/data/test.csv\")\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f20d5",
   "metadata": {
    "papermill": {
     "duration": 0.025991,
     "end_time": "2023-08-02T23:47:12.128675",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.102684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c372ed0",
   "metadata": {
    "papermill": {
     "duration": 0.02121,
     "end_time": "2023-08-02T23:47:12.158015",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.136805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896d289",
   "metadata": {
    "papermill": {
     "duration": 0.007909,
     "end_time": "2023-08-02T23:47:12.173598",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.165689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8a2d0",
   "metadata": {
    "papermill": {
     "duration": 0.042858,
     "end_time": "2023-08-02T23:47:12.224190",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.181332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n",
    "df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n",
    "\n",
    "print(\"Train Length Stat\")\n",
    "print(df_train[\"length\"].describe())\n",
    "print()\n",
    "\n",
    "print(\"Test Length Stat\")\n",
    "print(df_test[\"length\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f42c1",
   "metadata": {
    "papermill": {
     "duration": 0.007712,
     "end_time": "2023-08-02T23:47:12.240234",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.232522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n",
    "\n",
    "Note that all the tweets are in english."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a931b0",
   "metadata": {
    "papermill": {
     "duration": 0.007676,
     "end_time": "2023-08-02T23:47:12.255710",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.248034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f3f44",
   "metadata": {
    "papermill": {
     "duration": 0.017228,
     "end_time": "2023-08-02T23:47:12.280828",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.263600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAINING_EXAMPLES = df_train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 2\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfcaea",
   "metadata": {
    "papermill": {
     "duration": 0.031306,
     "end_time": "2023-08-02T23:47:12.319867",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.288561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "X_test = df_test[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c760da",
   "metadata": {
    "papermill": {
     "duration": 0.007655,
     "end_time": "2023-08-02T23:47:12.335535",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.327880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load a DistilBERT model from Keras NLP\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n",
    "\n",
    "The BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n",
    "\n",
    "We will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n",
    "\n",
    "It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "Specifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d4eda",
   "metadata": {
    "papermill": {
     "duration": 6.373231,
     "end_time": "2023-08-02T23:47:18.716546",
     "exception": false,
     "start_time": "2023-08-02T23:47:12.343315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load a DistilBERT model.\n",
    "# preset= \"distil_bert_base_en_uncased\"\n",
    "\n",
    "# # Use a shorter sequence length.\n",
    "# preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
    "#                                                                    sequence_length=160,\n",
    "#                                                                    name=\"preprocessor_4_tweets\"\n",
    "#                                                                   )\n",
    "\n",
    "# # Pretrained classifier.\n",
    "# classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "#                                                                preprocessor = preprocessor, \n",
    "#                                                                num_classes=2)\n",
    "\n",
    "# classifier.summary()\n",
    "\n",
    "from keras_nlp.models import BertPreprocessor, BertClassifier\n",
    "\n",
    "preset = \"bert_tiny_en_uncased\"\n",
    "\n",
    "preprocessor = BertPreprocessor.from_preset(\n",
    "    preset,\n",
    "    sequence_length=160,\n",
    "    name=\"preprocessor_4_tweets\"\n",
    ")\n",
    "\n",
    "classifier = BertClassifier.from_preset(\n",
    "    preset,\n",
    "    preprocessor=preprocessor,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03281335",
   "metadata": {
    "papermill": {
     "duration": 0.011343,
     "end_time": "2023-08-02T23:47:18.739662",
     "exception": false,
     "start_time": "2023-08-02T23:47:18.728319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train your own model, fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fda63-4750-473d-b41e-576b9e2155dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_core as keras\n",
    "\n",
    "\n",
    "classifier.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "history = classifier.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20664d",
   "metadata": {
    "papermill": {
     "duration": 233.46154,
     "end_time": "2023-08-02T23:51:12.212333",
     "exception": false,
     "start_time": "2023-08-02T23:47:18.750793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Compile\n",
    "# classifier.compile(\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n",
    "#     optimizer=keras.optimizers.Adam(1e-5),\n",
    "#     metrics= [\"accuracy\"]  \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Fit\n",
    "# history = classifier.fit(x=X_train,\n",
    "#                          y=y_train,\n",
    "#                          batch_size=BATCH_SIZE,\n",
    "#                          epochs=EPOCHS, \n",
    "#                          validation_data=(X_val, y_val)\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09107262",
   "metadata": {
    "papermill": {
     "duration": 0.050368,
     "end_time": "2023-08-02T23:51:12.306908",
     "exception": false,
     "start_time": "2023-08-02T23:51:12.256540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def displayConfusionMatrix(y_true, y_pred, dataset):\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true,\n",
    "        np.argmax(y_pred, axis=1),\n",
    "        display_labels=[\"Not Disaster\",\"Disaster\"],\n",
    "        cmap=plt.cm.Blues\n",
    "    )\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n",
    "    f1_score = tp / (tp+((fn+fp)/2))\n",
    "\n",
    "    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44e486",
   "metadata": {
    "papermill": {
     "duration": 22.191254,
     "end_time": "2023-08-02T23:51:34.538190",
     "exception": false,
     "start_time": "2023-08-02T23:51:12.346936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "\n",
    "displayConfusionMatrix(y_train, y_pred_train, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9bd27",
   "metadata": {
    "papermill": {
     "duration": 8.223166,
     "end_time": "2023-08-02T23:51:42.816866",
     "exception": false,
     "start_time": "2023-08-02T23:51:34.593700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_val = classifier.predict(X_val)\n",
    "\n",
    "displayConfusionMatrix(y_val, y_pred_val, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6015069-4d1a-4f52-b8ad-4c3d6427af15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 343.946818,
   "end_time": "2023-08-02T23:52:09.318853",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-02T23:46:25.372035",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
